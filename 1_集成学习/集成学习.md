# 集成学习

​     前言： 温习完决策树，想再学一遍随机森林，看到一个很好的视频，从另一个角度开启了我温习随机森林之旅。

**本文主要是一个系列视频的笔记，当然也找了些其他资料，综合成文**

> 视频：
>
> 1.https://www.bilibili.com/video/av38821041?from=search&seid=11414573078706162959

**集成学习的思想**：

（https://blog.csdn.net/xtingjie/article/details/73130629 ）

1. 集成学习（ensemble learning）是一种**分类方法**

2. 由众多分类器组成，如决策树、支持向量机、神经网络等等

3. 它的分类结果也是由众多分类器的分类结果表决得到，就像投票选举一样

4. 在一个集成学习器里面，可以全部是同一类分类器（如随机森林），也可以是不同一类的分类器

5. 由众多弱分类器组成一个强分类器，极大地减少了分类的失误率,只要平均每个分类器的正确率高于50%，而且分类器足够多，就一定能得到正确的分类结果

## Hard Voting

> 分类结果少数服从多数

## Soft Voting

(https://www.cnblogs.com/volcao/p/9483026.html)

> 1.很多情况下，仅仅少数服从多数，并不合理。这里，Soft将所有模型预测样本为某一类别的概率求平均值作为标准。
>
> 2.但1引如入了一个问题，要求集成的每个模型都可以估计概率,常用的分类器都可以计算概率吗？下面举例子：
>
> ​    （1） 逻辑回归本身就是概率模型
>
>    （2） KNN, 当确定K时，可以利用靠近样本个数/总的个数  考计算概率。
>
> ​    （3）决策树，叶子节点内的样本不会是纯的，所以也可以计算概率。
>
>    （4）SVM没有使用到概率，但也可以用SVC来估计概率（会牺牲计算时间），SVC默认参数不支持计算概率。

## Bagging

（相似文章：https://www.cnblogs.com/volcao/p/9486417.html）

> 1. 仅上面的思路也是不行的，为什么？
>
> 2. 因为虽然有很多机器学习方法，但是从投票的角度看，仍然不够多。
>
> 3. 所以，我们要创建更多的子模型！集成更多的子模型的意见！且子模型之间不能一致！子模型之间要有差异性。
>
> 4. 好，那现在如何能做到创建更多的互有差异的子模型呢？？
>
> 5. 一个简单的方法是：**每个子模型只看样本数据的一部分**
>
> 6. 但，问题又来了，每个子模型只选取一小部分样本，那得到的预估结果准确率会很低！！
>
> 7. 那为什么还有效？事实上这就是集成学习的威力，集成学习的每个子模型不需要太高的准确率。下图数字举例子：
>
>     

![1563286599458](/home/lixiang/.config/Typora/typora-user-images/1563286599458.png)

>    ​    应正了最开始那句化，我不多要求，假设每个子模型的预测准确率高于50%，那只要我的数量够多，最终结果也不会差。但是通常情况下，每个子模型预测概率都是或高或低，但实际效果也不差。

___



> 8. 每个子模型只取一部分数据，即每个子模型只看样本的一部分，这样可以制造差异性，但是，怎么看数据的一部分？
> 9. 这里有两种方式：（1）有放回取样 **bagging**（2）不放回  **Pasting**
> 10. 通常使用bagging的方式,为什么？优点：（1）样本过少时，仍然可以训练很多子模型。（2）不过度依赖随机划分，不会因如何划分数据而给结果带来太大的影响。
> 11. 统计学中有放回的取样：**bootstrap**



## OOB

> Out-of-Bag
>
> 1.有放回的取样导致一部分样本很有可能根本没有取到，平均大约有
>
> 37%的样本没有取到。
>
> 2.上面那些样本可以更加合理的利用，比如不再单独划分测试数据集，而使用这部分没有取到的样本做测试/验证。
>
> 3.集成的思路极易并行化处理。
>
> 4.能对样本进行随机采样，那也可以针对特征进行随机采样：Random Subspaces
>
> 5.既针对样本，又针对特征进行随机采样：Random Patches,即：行列随机，如下：

![1563537836977](/home/lixiang/.config/Typora/typora-user-images/1563537836977.png)



# 课程代码

> 1. 可以参考这位老兄的博客的系列博客笔记（也是这个课程的）：
>
>    https://www.cnblogs.com/volcao/p/9486417.html
>
> 2. 本人全部整理到了一起（jupyter）,但觉的没必要往上，毕竟也是看1的。
















































